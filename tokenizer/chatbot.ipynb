{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vei-liang.brandon-li.VITROX\\Desktop\\PythonProjects\\myenv39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\vei-liang.brandon-li.VITROX\\Desktop\\PythonProjects\\myenv39\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from transformers import TFBertForSequenceClassification, BertTokenizer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "model_save_path = 'model' \n",
    "tokenizer_save_path = 'tokenizer'\n",
    "label_encoder_save_path = 'label_encoder.pkl'  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating synthetic data w/ timestamps for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('chatbot4.csv')\n",
    "\n",
    "df_today = df.copy()\n",
    "df_today['sentence'] = df_today['sentence'] + ' today'\n",
    "\n",
    "df_this_month = df.copy()\n",
    "df_this_month['sentence'] = df_this_month['sentence'] + ' this month'\n",
    "\n",
    "df_this_year = df.copy()\n",
    "df_this_year['sentence'] = df_this_year['sentence'] + ' this year'\n",
    "\n",
    "extended_df = pd.concat([df_today, df_this_month, df_this_year], ignore_index=True)\n",
    "\n",
    "extended_df.to_csv('data_with_time.csv', index=False)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|██████████| 440M/440M [00:38<00:00, 11.6MB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\vei-liang.brandon-li.VITROX\\Desktop\\PythonProjects\\myenv39\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:From c:\\Users\\vei-liang.brandon-li.VITROX\\Desktop\\PythonProjects\\myenv39\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "83/83 [==============================] - 327s 4s/step - loss: 0.6296 - accuracy: 0.8149 - val_loss: 0.1153 - val_accuracy: 0.9726\n",
      "Epoch 2/3\n",
      "83/83 [==============================] - 312s 4s/step - loss: 0.0942 - accuracy: 0.9817 - val_loss: 0.0268 - val_accuracy: 0.9970\n",
      "Epoch 3/3\n",
      "83/83 [==============================] - 320s 4s/step - loss: 0.0417 - accuracy: 0.9939 - val_loss: 0.0260 - val_accuracy: 0.9970\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2ce35def3d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('chatbot4.csv')\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(df['label'])\n",
    "\n",
    "num_unique_labels = len(label_encoder.classes_)\n",
    "\n",
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(\n",
    "    df['sentence'].tolist(),\n",
    "    encoded_labels,\n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=encoded_labels \n",
    ")\n",
    "\n",
    "train_encodings = tokenizer(train_sentences, padding=True, truncation=True, max_length=128, return_tensors=\"tf\")\n",
    "val_encodings = tokenizer(val_sentences, padding=True, truncation=True, max_length=128, return_tensors=\"tf\")\n",
    "\n",
    "train_labels_onehot = tf.keras.utils.to_categorical(train_labels, num_classes=num_unique_labels)\n",
    "val_labels_onehot = tf.keras.utils.to_categorical(val_labels, num_classes=num_unique_labels)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), train_labels_onehot)).shuffle(len(train_sentences)).batch(16)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((dict(val_encodings), val_labels_onehot)).batch(16)\n",
    "\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_unique_labels)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "model.fit(train_dataset, validation_data=val_dataset, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.save_pretrained(model_save_path)\n",
    "\n",
    "tokenizer.save_pretrained(tokenizer_save_path)\n",
    "\n",
    "with open(label_encoder_save_path, 'wb') as file:\n",
    "    pickle.dump(label_encoder, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\vei-liang.brandon-li.VITROX\\Desktop\\PythonProjects\\myenv39\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFBertForSequenceClassification.from_pretrained(model_save_path)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(tokenizer_save_path)\n",
    "\n",
    "with open(label_encoder_save_path, 'rb') as file:\n",
    "    label_encoder = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class for the sentence is: sales_report with a confidence score of 0.9802234172821045\n"
     ]
    }
   ],
   "source": [
    "def predict_class(sentence, model, tokenizer, label_encoder):\n",
    "    inputs = tokenizer(sentence, padding=True, truncation=True, max_length=128, return_tensors=\"tf\")\n",
    "\n",
    "    logits = model(inputs.data).logits\n",
    "\n",
    "    probabilities = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "    predicted_class_idx = tf.argmax(probabilities, axis=-1).numpy()[0]\n",
    "\n",
    "    confidence_score = probabilities[0, predicted_class_idx].numpy()\n",
    "\n",
    "    predicted_class = label_encoder.inverse_transform([predicted_class_idx])[0]\n",
    "    \n",
    "    return predicted_class, confidence_score\n",
    "\n",
    "predicted_class, confidence_score = predict_class(\"who is our worst employee\", model, tokenizer, label_encoder)\n",
    "if confidence_score < 0.70 :\n",
    "    predicted_class = \"ambiguous\"\n",
    "print(f\"The predicted class for the sentence is: {predicted_class} with a confidence score of {confidence_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
